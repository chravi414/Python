{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for Preprocessing \n",
    "\n",
    "# --------method to remove punctuations from the text\n",
    "def remove_punct(text):\n",
    "    no_punct=\"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "# ---------method to remove stopwords from the data text\n",
    "def stopwords_remove(text):\n",
    "    words=\"\".join([w for w in text if w not in stopwords.words('english')])\n",
    "    return words\n",
    "\n",
    "#--------- Lemmatizing to shorten the words to root form\n",
    "def lemmat_word(text):\n",
    "    lem_text=\"\".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data\n",
    "data = pd.read_csv(\"spam.csv\",encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "data['Text']=data['Text'].apply(lambda x:remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['Text']=data['Text'].apply(lambda x:tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Removal\n",
    "from nltk.corpus import stopwords\n",
    "data['Text']=data['Text'].apply(lambda x:stopwords_remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "data['Text']=data['Text'].apply(lambda x:lemmat_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>gojurongpointcrazyavailablebugisngreatworldlae...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>oklarjokingwifuoni</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>freeentry2wklycompwinfacupfinaltkts21stmay2005...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>udunsayearlyhorucalreadysay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nahdontthinkgoesusflivesaroundthough</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                               Text Unnamed: 2  \\\n",
       "0   ham  gojurongpointcrazyavailablebugisngreatworldlae...        NaN   \n",
       "1   ham                                 oklarjokingwifuoni        NaN   \n",
       "2  spam  freeentry2wklycompwinfacupfinaltkts21stmay2005...        NaN   \n",
       "3   ham                        udunsayearlyhorucalreadysay        NaN   \n",
       "4   ham               nahdontthinkgoesusflivesaroundthough        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Class'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing TFIDF Vectorization\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "\n",
    "# Creating the Instance of TFIDF vectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "# Training the Data\n",
    "X_train_tfidf = vect.fit_transform(X_train)\n",
    "X_test_tfidf= vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer score: 0.8672645739910314\n",
      "Confusion Matrix is : \n",
      " [[965   0]\n",
      " [148   2]]\n"
     ]
    }
   ],
   "source": [
    "# Model Implementation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Creating instance of model\n",
    "clf= MultinomialNB()\n",
    "\n",
    "# Training the Model\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting the result\n",
    "predicted  = clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Calcluating the Accuracy\n",
    "from sklearn import metrics\n",
    "score = metrics.accuracy_score(y_test, predicted)\n",
    "print(\"TfidfVectorizer score: \" + str(score))\n",
    "\n",
    "# Building Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix is : \\n\",confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
